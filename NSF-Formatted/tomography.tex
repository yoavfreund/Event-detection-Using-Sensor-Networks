\begin{figure}[t]
\centering
\includegraphics[width=6cm]{figs/patch.png} 
\caption{2D velocity image corresponds to  map divided into pixels (dashed boxes). The square image patch $i$ contains $n$ pixels. $W_1$ and $W_2$ are the vertical and horizontal dimensions of the image (in pixels), which give $I$ unique patches. Sensors are shown as red x's and the ray paths between the sensors are shown as blue lines.}
\label{fig:patchMap}
\end{figure}
\subsection{Tomography}
%\yoav{I believe you are talking here about tomography, or reconstructing the environment. Can you write a paragraph of introduction, what is the problem? What is the desired solution?}
we develop separately two slowness models, deemed the \textit{global} and \textit{local} models,  and briefly discuss dictionaries for sparse modeling. The global model considers the larger scale or global features and relates travel times to slowness. The local model considers smaller scale or more localized features with sparse modeling. 

In the global model, slowness pixels (see Fig.~\ref{fig:patchMap}(a)) are represented by the vector $\mathbf{s'=s}_\mathrm{g}+\mathbf{s}_0\in\mathbb{R}^{N}$, where $\mathbf{s}_0$ is reference slownesses and $\mathbf{s}_\mathrm{g}$ is perturbations from the reference, here referred to as the {\it global slowness}, with $N=W_1W_2$. Similarly, the travel times of the $M$ rays are given as $\mathbf{t'=t+t}_0$, where $\mathbf{t}$ is the travel time perturbation and $\mathbf{t}_0$ is the reference travel time. The tomography matrix $\mathbf{A}\in\mathbb{R}^{M\times N}$ gives the discrete path lengths of $M$ straight-rays through $N$ pixels (see Fig.\ \ref{fig:patchMap}(a)). Thus  $\mathbf{t}$ and $\mathbf{s}_\mathrm{g}$ are related by the linear measurement model
%
%\begin{linenomath*}
\begin{equation}
\mathbf{t}=\mathbf{As}_\mathrm{g}+\epsilon,
\label{eq:linearTraveltime}
\end{equation}
%\end{linenomath*}
%
where $\mathbf{\epsilon}\in\mathbb{R}^M$ is Gaussian noise $\mathcal{N}(\mathbf{0},\sigma_\epsilon^2\bf{I})$, with mean $\mathbf{0}$ and covariance $\sigma_\epsilon^2\bf{I}$. We estimate the perturbations, with $\mathbf{s}_0$ and $\mathbf{t}_0=\mathbf{A}\mathbf{s}_0$ known. We call (\ref{eq:linearTraveltime}) the \textit{global model}, as it captures the large-scale features that span the discrete map and generates $\mathbf{t}$. 

\begin{itemize}
\item If using EM signals where the wave speed does not vary much the above formulation could be modified to do attenuation tomography.

\item For unknown sensor location, only a graph of the sensor response is obtained. 
\end{itemize}
\subsection{Tomography}
\yoav{I believe you are talking here about tomography, or reconstructing the environment. Can you write a paragraph of introduction, what is the problem? What is the desired solution?}
Sparse modeling assumes that signals can be reconstructed using a few (sparse) vectors, called atoms, from a potentially large set of atoms, called a dictionary. Recent ocean acoustics works utilizing sparse modeling is beamforming\cite{Xenaki2014}, matched field processing \cite{Gemba2017}, and geoacoustic inversion \cite{gerstoft2018}. One challenge in sparse modeling is finding the best dictionary for sparsely representing specific signals. Such dictionaries can be composed of wavelets, or the discrete cosine transform (DCT). These predefined dictionaries perform well for many signals. However, using a form of unsupervised machine learning, called dictionary learning, optimal dictionaries can be learned directly from specific data\cite{mallat1999}. It has been shown that learned dictionaries outperform generic dictionaries when sufficient signal examples are available. Machine learning, and specifically dictionary learning, have recently obtained compelling results in ocean acoustics \cite{Bianco2017} and seismology\cite{kong2018}. 
\rayan{I might be able to throw in some text about learning fast dictionaries, i.e., dictionaries learned from the data, but that also admit fast transforms like FFTs. Peter, Yoav, what do you think?}

\yoav{I don't understand the following three paragraphs, can you give some technical details? Formulas?}
In current work, we have developed a machine learning-based travel time tomography method called locally sparse travel time tomography (LST)\cite{bianco2018}. In LST, small scale local features contained in small rectangular groups of pixels, called patches, in an overall slowness (inverse  speed) map are constrained using a sparse model. Further, the sparsifying dictionary is adapted to the specific slowness data using dictionary learning. Larger scale, or global features spanning the  map, are constrained with least-squares regularization. Unlike conventional tomography, in which model features are forced to be exclusively smooth or discontinuous, the LST approach permits smooth and discontinuous local features via dictionary learning. 


Whereas many machine learning techniques in geoscience\cite{kong2018}, are reliant on large amounts of training data, LST requires none. In LST we adopt the adaptive dictionary learning paradigm from image denoising \cite{elad2010} and medical imaging\cite{ravishankar2011}, in which dictionaries are learned directly from patches of the corrupted image. In LST, slowness dictionaries are learned from patches of a least squares regularized inversion, and are then used to reconstruct a sparsity-constrained slowness image. Assuming sufficiently dense ray sampling, the dictionary is initially unknown and is learned in parallel with the inversion. LST obtains high resolution by assuming that small patches of discrete slowness maps are repetitions of few elemental patterns from a dictionary of patterns. These patterns, which are described by the atoms in the dictionaries, are extracted from the data by dictionary learning. The increase in performance for synthetic slownesses relative to competing methods, are demonstrated for  ambient noise tomography\cite{bianco2018}. 

Assuming that the travel paths between sensors has been estimated\cite{sabra2005,gerstoft2006} We here propose the future development of machine learning-based tomography methods in ocean acoustics. Such methods will help to more fully-exploit both existing hydrophone and environmental data, as well as very dense sampling from future arrays with many sensors. Such large scale, mobile, and deformable arrays, will use ambient noise processing \cite{sabra2005}, to obtain very dense and rich data sets. We propose for future work to: (1) further develop a dictionary learning-based travel time tomography \cite{bianco2018}, accounting for uncertainty in the measurements and physics; (2) formulate the dictionary learning-based approach as CNN via CSC; and (3) apply this CSC tomography framework to  data assimilation, to obtain higher-resolution estimates of water column parameters over conventional methods. We further propose to develop  (5) acoustic event detection methods that leverage recent advances in machine learning.

